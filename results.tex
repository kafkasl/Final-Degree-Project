\chapter{Results}


This section describes the results of refactoring pyProCT with pyCOMPSs. I divided into three subsections. The first one reports the benefits of using pyCOMPSs programming model. The second subsection contains the performance analysis of the results. Finally, the last part deals with tools that the framework offers.

\section{Programming Model}

pyCOMPSs framework is supposed to require little extra code to be used so first I will compare difference of size between each version. Figure \ref{tab:sizes} shows the comparison between the different classes required. Most of them are equal for the sequential, MPI and multiprocessing schedulers so they are grouped by the name \textit{original}. However MPI requires changes to some classes so they have a column of their own whenever they differ from the sequential and parallel versions. It's based on the number of characters and lines because python conventions encourage the usage of line breaks so the results could be misleading (some functions have one parameter per line).


\begin{center}
	\begin{tabular}{| R{6cm} | L{3cm} | L{3cm} |}
		\hline
		Class Name & Original & MPI & pyCOMPSs \\ 
		\hline
		\hline
		Driver & 169 / 8180  &  165 / 7911 \\
		\hline
		ClusteringProtocol & 78 / 3703 & 73 / 3490 \\
		\hline
		PostProcessingDriver & 32 / 1531 & 56 / 2700 \\
		\hline
		ClusteringExplorer & 195 / 8470 & 197 / 9226 \\ 
		\hline
	\end{tabular}
	\captionof{table}{Size comparison of duplicated classes} 
	\label{tab:sizes}
\end{center}


We see that the refactor did not add too much space. The driver and protocol classes are in fact shorter. This is caused because of the removal of the task-adding loop and the pyScheduler initialization. Postprocessing driver is longer due to the fact that on the original version this section is not parallel. The other ones are quite even.

PyCOMPSs framework just uses python decorators and API calls so, why do we observe a size increase in some classes? This is due to the fact that our data can not automatically serialized by python's pickle. Almost all the extra size is linked to the work needed to handle the matrix data. However, other than adding this little size overhead, the code is much clean and easy to read. 

Another important aspect is the execution process and tools offered by the framework. It is here were COMPSs truly shines. 

The level of hardware abstraction provided by the framework is really good. To execute pyProCT in a local environment one just needs to provide the language (python on this case), classpath, executable and parameters. If the user desires to customize the framework offers two kind of hardware configurations. On the one hand we find the \textit{resources.xml}. This file allows to define the workers to be used. This includes supercomputer nodes, cloud services, remote images and more. On the other hand we have the \textit{project.xml} which selects which of the defined resources are to be actually used and some runtime parameters. 

With this simple two files we can define a wide range of available resources to be used and then select which ones we want to use for an specific execution. Thanks to this we can use a lot of different resources without worrying about the internals and communications. COMPSs' already implements all the connectors required to use them so we just need to give a description of them, select which to use and decorate our code. 

For MareNostrum III executions this process is even easier. The development team has created scripts to submit jobs to the supercomputer. By default, it requires the same parameters as a normal execution. However, it has a wide range of easy-to-use options with a clear description. With all the parameters available, such as the network and file system to be used or the number of nodes, it automatically creates the \textit{resources.xml} and \textit{project.xml} that best suit our needs. 

This is tools are really useful. Most of the time working of this project has been spent on trying to execute the program on the supercomputer. One needs to understand how a submission queue system works, which parameters need to be specified, how the class paths are read and used, which way the supercomputer access files, which kind of problems may arise from mutex access to the datasets and so on. This is a quite daunting task for someone without advanced knowledge on the subject or with no one to ask help to. 

As stated, COMPSs required number of parameters are far less. Understanding some of the aforementioned things will help the user to better use the framework but they are not really mandatory because the COMPSs manuals are good and clear. Following the examples is enough to execute your own programs. 




\section{Performance}


COMPSs is designed to harness all the power of supercomputers and use all the available computing resources. The whole initialization of such a framework is not a trivial matter and because of that I expect it to be slow with respect to MPI or OpenMP. However, being designed to run programs with humongous datasets and computation times, this initial overhead is negligible. I expect to see that MPI and multiprocessing schedulers are faster than pyCOMPSs on small datasets and slower on larger ones. 

The performance has been measured with three datasets:

\begin{itemize}
\item [Small []]
\end{itemize}


\section{Tools}