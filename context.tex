\section{Context}


Nowadays the amount of digital information available is exponentially increasing. Just on 2015 we generated almost 8.000 exabytes of information. Facebook generates 105 terabytes of data each half hour, more than 48 hours of video per minute are uploaded to youtube and google has at least 1 million queries/minute. But why do we observe this massive increase? To start the cost of creating, managing and storing information has dramatically dropped: EMC Corporation estimates that on 2011 this cost has been cut to a 1/6 of what it was on 2005. But more importantly people is more connected than it has ever been; mobiles, websites and social channels are just some examples of a whole new world of data-generating people interactions. 

In this scenario is where we found the hot topic of today: Big Data. So what is it?, usually the term is used referring to data sets too big or complex to be processed with traditional data applications or on-hand management tools. According to the IT giant Gartner, Inc Big Data can be characterized by the "3 Vs", velocity, volume and variety \cite{Laney}:

\begin{quotation}
"Big data" is high-volume, -velocity and -variety information assets that demand cost-effective, innovative forms of information processing for enhanced insight and decision making.
\end{quotation}


However all this raw information needs to be processed and categorized in an effective way before being used. Cluster analysis methods are one of the most used tools to address this issue. 

The term \textbf{cluster analysis} (first used by Trion, 1939) refers to the task of sorting similar objects of a data set into groups (called clusters) in a way that the degree of similarity between each pair is maximal if they belong to the same cluster and minimal otherwise. Data sets can be imagined as points in a multidimensional space, where each feature of an object would represent a dimension. The CA methods need to identify, as efficiently as possible, the denser areas and group the into clusters.

Thanks to the clustering we can reduce the size of large data sets by extracting the most relevant information, usually the common features of a group or a subset of representatives. Cluster analysis (CA from now onwards) techniques thrive in the Big Data world because it's not feasible to manually label objects, there is no prior knowledge of the number and nature of the clusters and, also, their identifying traits may change over time. 

It is important to note that cluster analysis it's not an specific algorithm but rather the general task to perform. Due to the fact that the similarity criteria it's subjective and can change a lot between data sets, there isn't an optimal clustering algorithm. This is the reason why there are so many  clustering algorithms, each with it's advantages and inconveniences. Each algorithm uses it's own kind of cluster model that defines how the algorithm groups the items and defines the clusters. Some of the most relevant examples are:

\begin{description}
\item [Hierarchical Clustering Analysis (HCA)] \hfill \\ 
These methods seek, as their name indicates, to build a hierarchy of clusters. These can be done by starting with all elements in one clusters and the divide them in a "top down" way, this method is called \textbf{Divisive}. Opposed to this one we find the \textbf{Agglomerative} method, where each data point starts in a different cluster merging them as one moves up the hierarchy.
\item [Centroid Clustering,] \hfill \\ 
On these algorithms the similarity between different clusters is defined as the similarity between their centroids. \textbf{K-means} clustering is one of such methods, on it, each observation belongs to the nearest centroid which, in turn, serves as the representative or prototype of the cluster.
\item [Distribution-based Clustering] \hfill \\ 
Clusters are modelled by statistical distributions. On this category falls the well-known \textbf{expectation-maximization (EM) algorithm} which uses multivariate normal distributions.
\item [Density Clustering,] \hfill \\ 
These methods follow the intuitive notion, described earlier, of considering the observations as clouds of points in a multidimensional space and so, they identify clusters as connected dense regions in the data space. \textbf{DBSCAN} is one of such algorithms and it's both one of the most common as and most cited in scientific literature. 
\end{description}


It is also possible to classify clustering methods by some other properties such as:

\begin{description}
\item [Hard Clustering,] \hfill \\ where each element belongs to a cluster or not.
\item [Soft Clustering,] \hfill \\ where each element has likelihood of belonging to a certain cluster.
\end{description}




Cluster analysis methods can be applied to a wide range of subjects. Basically it can be used in any context where finding groups in sets of data is useful, for example:

\begin{description}
\item [Image segmentation,] \hfill \\ dividing an image into clusters or, more appropriate on this case, regions enhances a number of computer vision methods. Some examples are border detection or object recognition. \cite{Ayech2015}
\item [Market analysis,] \hfill \\ grouping enterprises \cite{Burca2014} or consumers \cite{Muller2014} to perform better market analysis or custom ads for each kind of consumer.
\item [Education tracking,] \hfill \\ grouping students to keep track of their record and apply more custom techniques to each student needs. \cite{Chan2014}
\item [Mathematical chemistry,] \hfill \\ to analyse, group and find structural similarities in chemistry compounds, minerals, and any kind of material for which a chemical analysis is convenient. \cite{Cortes2007}
\end{description}


\section{Stakeholders}

The implied stakeholders here are, primarily the COMPSs developer team and, second, the rest of the research teams implied on GRID and cluster execution platforms interested on Cluster Analysis. They are the ones who will mostly benefit from this project. As stated earlier, this project wants to develop and explore the COMPSs framework. With this the programming model will be refined and, thanks to it's usage, more desired features can be found. Also the future usage of pyProCT will help to "advertise" and enhance it's possible diffusion, which is the main goal to keep this kind of programming models and frameworks, not just alive, but on a develop and improvement route.

On the other hand the pyProCT software was originally intended for protein clustering. However, the program core is quite generic. Thanks to the implementation of new plug-ins and modules to load, transform and use other kind of data inputs, the software is expected to suit a large group of researchers without too much work. Providing support for other usages is not part of this project but a possible speed up and performance enhancement will help them all and so, they are considered the main clients of the software itself and they will benefit from it.

\section{State of the Art}


Most of the clustering analysis methods are not new, however with the dramatical increase in data size mentioned earlier, researchers have focused on improving their performance as much as possible. From this need arise new, but more rough, methods such as \textbf{canonpy clustering} \cite{Nayak2015} which can process huge amounts of information but it just a pre-partitions data to then analyse smaller partitions with slower methods.

The increasing amount of information each data point contains it's also a problem for some algorithms. This information leads to high-dimensional data which, in turn, causes problems to a big part of the modern algorithms. This is known as the curse of dimensionality, which basically points out the fact that high-dimensional data often becomes sparse due to the large volume of space. It is important to note that this problem is not due to data itself but to the algorithm used. Some modern approaches try to overcome this difficulty by aiming to reduce the data-dimensionality, with  methods such as \textbf{principal component analysis} \cite{Kupski2015}, use just some part of it, like in \textbf{subspace clustering} \cite{Adler2015} which have adopted ideas from density-based algorithms.

Apart from the clustering analysis techniques the focus of the project is the COMPSs refactoring. The election of the framework has been made according to two reasons: one, the proximity of the BSC research team which will ease the development, two, and most important, the framework is aimed to distributed computing on which the CA tasks are best executed. 

%\section{Context}
%
%Cluster analysis methods can be applied to a wide range of subjects. Basically it can be used in any context where finding groups in sets of data is useful, for example:
%
%\begin{description}
%\item [Image segmentation,] \hfill \\ dividing an image into clusters or, more appropriate on this case, regions enhances a number of computer vision methods. Some examples are border detection or object recognition. \cite{Ayech2015}
%\item [Market analysis,] \hfill \\ grouping enterprises \cite{Burca2014} or consumers \cite{Muller2014} to perform better market analysis or custom ads for each kind of consumer.
%\item [Education tracking,] \hfill \\ grouping students to keep track of their record and apply more custom techniques to each student needs. \cite{Chan2014}
%\item [Mathematical chemistry,] \hfill \\ to analyse, group and ifnd structural similarities in chemistry compounds, minerals, and any kind of material for which a chemical analysis is convenient. \cite{Cortes2007}
%\end{description}

%
%\section{pyProCT}
%
%pyProCT is a clustering analysis software. As we stated earlier, correct usage of clustering analysis methods is not easy: correct algorithm selection, better parameters estimation or appropriate result analysis are just some of the problems that CA tools user faces. However, we also showed that CA is present in a lot of different subjects and is used, or would be useful, to people with a quite limited knowledge both on algorithmic methods and statistics. On the other way around we also find that CA specialists may not be able to correctly assess the results of a clustering due to the nature of the data itself.
%
%pyProCT aims to close a bit this gap. To achieve this it uses five different algorithms and allows the user to define a clustering goal or hypothesis. With this and some more options, most related to the parameter estimation of each method, it tries to find the best algorithm and it's parameters for the inputted data. This way, through a more "semantic" approach we expect to guide pyProCT to find the combination that will produce the best clustering, without forcing the user to deeply understand the pros and cons each method w.r.t to an specific kind of data.
%
%
%
%MORE DESCRIPTION
%
%\subsection{Algorithms}
%
%pyProCT uses the following five algorithms to find the best clustering. It also features a last one which clusters the data randomly. This one is used for comparative purposes so it won't have more consideration that the utility it provides for other's behaviour analysis.
%
%\begin{enumerate}
%
%\item{K-medoids}
%\item{Hierarchical}
%\item{DBSCAN}
%\item{GROMOS}
%\item{Spectral}
%
%\end{enumerate}
%
%

