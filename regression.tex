\chapter{pyProCT-regression}


pyProCT-Regression is the software designed to validate the pyCOMPSs refactor code. It implements the so-called black-box validation method. The validator will take a list of tests to perform. First we need to generate the expected results with the original version or pyProCT, then we'll run the same tests with the new version and make sure that the output matches the expected results.


pyProCT results depend on the parameters defined on the control script because we can select which results to save, which format, wether we want to save the computed matrix (and it's image) and so on (see \hyperref[sec:execution_flow]{Section \ref{sec:execution_flow}} for more info). Because of that the validator needs to be flexible, allowing to define more or less files to check. On the other hand, we have different two different test scenarios: one, validating the results of the original version against the refactored one, two, validate the new scheduler against known results of the other schedulers.

To achieve this behaviour Regression takes a test list as input with all the information it needs to check for each test scenario. 
Each test has a name and description and the following attributes.

\begin{description}
\item [Name,] a unique test name.
\item [Description,] an small description of the test.
\item [Script,] defines the input script for the pyProCT execution. 
\item [Expected results dir,] is the folder containing the expected output and the files specified on files\_to\_check
\item [Files to check:] a list of the additional files that regression will check, together with the default ones: test.out and test.err.
\end{description}

If we run the tester with the "GENERATE" option it will, for each test, run the installed pyProCT with the defined control script and save the normal standard output and error as well as the "files to check" on the "expected results dir".

On the other hand, running the tester with "TEST" will also run the control script with the installed pyProCT but after that it will check that the generated output matches the content of the "expected results dir".

The last three attributes allow us to use a single expected results dir with different scripts and schedulers, or use the new version of pyProCT with the same tests but on testing mode to validate the refactor. When validating that the original schedulers work as expected I also add other files to check such as the parameters.json and the clustering folders (containing information about the generated clustering).

\section{Basic tests and issues}

The basic tests validate each of the four sections of pyProCT (global, data, clusering, postprocess) incrementally. However once I started generating the MPI results I faced the first issue: MPI (and later also pyCOMPSs) scheduler need to be called with mpirun and runcompss. 

To solve it I could make all the schedulers work with the same call or adapt the tester to each scheduler (reading that information from the control script). I decided on the first because the scheduler is set on the control data (so the main file can act as a switch performing the runcompss of mpirun if needed) and pyProCT will be easier to use. Now the main file calls the bash script with the runcompss [params] and mpirun [params]. This way all the versions work same way and the tests on Regression just need to change the control script.

Once done this modifications it was easier to write the tests. Before starting the refactor I generated all the expected results. I tested this results with the same code that generated them to make sure the tester worked but some of them failed because of the second, and expected, issue: the random initializations of some algorithms. To solve this I did the same that on tic-tac-toe, albeit a bit more complex: "set the seeds" for the algorithm's initializations removing the stochastic and non-deterministic parts.


Afterwards, for each scheduler I ran the basic tests, first with the original pyProCT, then with the refactor. However the pyCOMPSs mode was not available before the refactor so I validated it's output against the expected results of the MPI (could be any of the others). pyCOMPSs embeds the application output on its own so, in this case, the tester checks if the expected output is contained within the pyCOMPSs one.

Finally for each major modification of pyProCT I ran this suite of tests to validate the work done.
