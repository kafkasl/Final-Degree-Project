\chapter{Appendices}

\section{Performance Tools Research}
\label{chap:ttt}

\subsection{Motivation}

%TODO rewrite to make it the first appendix
After the familiarization with pyProCT it is time to explore its scheduler and to find the tools necessary to analyze its performance. I wrote an small sequential testing program in order to test it all on a much smaller scale. The next sections' decriptions rely on it to explain the scheduler and tools. More precisely, first section will describe the program to put the reader in context. On the second section I will parallelize the program with the scheduler. Last two sections will describe the analysis process: first the instrumentation for generating traces and then the visualization tool. During the process I will try to find possible problems that may arise due to the parallelization, like the reproducibility on random algorithms or results' validation, which could complicate the posterior work on pyProCT.


\subsection{Test program description}

The developed program was an command-line implementation of the popular game Tic-tac-toe. It suited quite well the project needs because it featured good parallelization options, stochastic elements, a clear turn structure (easing up the first analysis trials), variable-size parametrized tasks and automatized execution (having two AI playing).

\hyperref[fig:tic-tac-toe]{Figure \ref{fig:tic-tac-toe}} shows a simplified version of the program's execution flow. All the logic handling which player's turn is, how the board is marked and some other parts have been omitted because they are not relevant. 

For each turn, while the game is not finished, the program calls the \textit{Player} class' \textit{play()} function. The algorithm implementing the AI moves is a montecarlo-like method. It randomly simulates a big number of games for each available cell and then choses the cell with the best score.

To do this it first gets all the free cells, then for each available one it calls the exploration\_handler method. This method in turn calls mark\_an\_explore a number of times (this number is defined by the ITERATIONS parameter which is a command-line argument) with a copy of the game board. Basically, mark\_and\_explore first the cell passed as parameter and then fills randomly the copied board, till the game is finished (either by a player winning or a draw), returning the cell and the id of the winning player or a 0 if there is a draw. The list of winnig id's is then returned to the montecarlo function. Afterwards we initialize the score value for each available cell with infinity. Then for each tuple containing a cell and the winner of that simulation we increment the score of the cell if the player has won or decrease it if the player has lost (the algorithm could modify the score for draw results). The actual increment/decrement values can be tuned but it's not relevant for testing purposes. 

\subsection{pyScheduler refactor}

Once the sequential program was ready, next step was to decide how to refactor it with pyScheduler (pyProCT's scheduler). The decision was to consider each \textit{exploration\_handler()} function as a task to see how their size affects the performance of the scheduler. The ITERATIONS parameter allows to change the number of iterations performed inside \textit{exploration\_handler()} so this was deemed the best option. 

The selected scheduler has three scheduling types, one sequential and two parallel: 

\begin{description}
\item [Serial,] which executes the task sequentially.
\item [ProcessParallelScheduler,] which uses python's multiprocessing module.
\item [MPIParallelScheduler,] which uses mpi4py for the parallelization.
\end{description}

The usage of the scheduler is simple. For each task we have to call the \textit{add\_task} method providing the following information:

\begin{description}
\item [Task name:] a unique task name.
\item [Dependencies:] a list of this task dependencies (which must be a list of other tasks names).
\item [Description:] a description of the task.
\item [Target function:] the name of the function to be executed.
\item [Function kwargs:] the list of the keyword arguments which need to be passed to the target function.
\end{description}

Once the task list is completed we just need to call the \textit{run()} method of the scheduler. The method will return a list with the execution results of each task.

For the tic-tac-toe these are the values for the queued tasks:

\begin{description}
\item [Task name:] ExplorationXY (being X, Y the coordinates of the cell to be explored).
\item [Dependencies:] [ ] (the empty list because there are no dependencies among different explorations).
\item [Description:] Montecarlo exploration.
\item [Target function:] self.exploration\_handler (as we are inside Player class namespace we must add the self).
\item [Function kwargs:] {"x": x, "y": y, "board": board} (being X, Y the coordinates of the cell to be explored, and board the current state of the game).
\end{description}

\subsection{Instrumenting with Extrae}

Next step was to start using the analysis tools. The decision was to use the  \textbf{Extrae + Paraver} combination. Extrae \footnote{ Find extrae documentation on the performance tools section of \ref{sec:docs} .Documenation} is the package used for instrumenting the code; Paraver \footnote{ Find paraver documentation on the performance tools section of \ref{sec:docs} .Documenation} is the tool used to visualize the traces generated by Extrae. These tools have been both developed at the BSC to be used together. We chose them because of the Extrae support to python, the offered assistance and proximity of the tools' experts and the fact that they are both installed and configured on MareNostrum III, which is our target execution platform.

Extrae offers two different ways to instrument the code: automatically instrument functions (providing a list of functions to the extrae XML configuration file) or including the extrae module (import pyextrae)  and emit specific events inside the code with \textit{pyextrae.eventandcounters(type, value)}. 

The basic usage of the first, which  does not require any changes on the code, instruments the entry and exit points of the functions. More complex behaviours are also available but for these tests the basic one is enough. 

The second one just needs to add the mentioned function call wherever we are interested to emit an event.

To start, I set \textit{play, montecarlo} and \textit{exploration\_handler} methods to be automatically instrumented and added a two events: one before the scheduler intialization and task addition and one just after the scheduler \textit{run()}. On the sequential version \footnote+{ From now, "sequential" will refer to the schedulerless version, "serial" to the one with the sequential scheduler, "parallel" for the one using ProcessParallel and "mpi" for the mpi4py one} this corresponds to before and after looping through the available cells (calling \textit{exploration\_handler} on each iteration).

Support for python is only available from version 3.0 onwards and it's not fully tested so we faced some problems. For the sequential and serial versions everything went well, the traces  were correct and they could be visualized with Paraver. For the parallel version we were not able to extract correct traces as extrae was not able to detect the parallelization method. When I tried the MPI version it didn't work either for two reasons. On one hand, adding the user functions' automatic instrumentation made the whole execution to end with a segmentation fault without generation the traces. On the other hand, using just the event emit method, the visualization showed just one thread. 

After meeting with BSC people we managed to solve the issue. The problem was that extrae used a sequential-tracing library, so it could not detect the mpi multiple threads. To solve it we substituted this sequential library with an mpi-tracing one. After some work on their part the first issue was also solved by changing some values on \textit{Extrae\_define\_event\_type}. 

For the parallel implementation with multiprocessing this approach wasn't supported. They gave me some ideas to try but to no avail. I linked extrae with a number of different libraries, such as the pthreads one, to see it they were able to hook themselves to the python multiprocessing threads but it didn't work. I left the issue open and went forward. Fortunately, after working on the new tracing system for pyCOMPSs at the BSC, I managed to develop a workaround tracing system albeit quite more rudimentary and inconvenient to use. Extrae has a command line usage of which I used two basic commands:

\textit{extrae-cmd init node slots}

\textit{extrae-cmd emit slot event\_value event\_type}, 

I created a Python class wrapper for this two commands. This way I reused it to instrument the pyProCT later. This class deals with the extrae paths, concurrency as well as providing a easier interface to call from python. To use it first it is necessary to initialize each used node with an ID and the number of processes/threads it will contain. Then for each event we want to emit we specify it's ID (which must be positive and smaller than the number of threads we set for that node) and the value and type of the event. 

The first trials I dit raised a segmentation fault; knowing that emitting an event with an out-of-range thread ID raises a segmentation fault I figured out that the initialization was not correct. I tried a number of different methods. Because this extrae usage is not the recomended nor normal approach there is no documentation for it nor examples for it. After several days working on it I resolved to meet with the extrae team. Working with them we found out that the segmentation fault was caused by a bug on the release I was using. Kindly they fixed it and made a custom package for me to use. With it I finally was able to achieve a basic instrumentation for the parallel (with python-multiprocessing) version. This is limited to emit events and can not produce the advanced visualizations and results achieved on the serial and MPI version.

\begin{landscape}
\begin{figure}
\includegraphics[width=20cm]{img/tic-tac-toe.png}
\caption{Tic-tac-toe Execution Flow}
\label{fig:tic-tac-toe}
\end{figure}
\end{landscape}


\subsection{Visualizing with Paraver}


Once generated the trace file the next step is to analyze them with the visualization tool Paraver.

First thing was to create a configuration file that would show the instrumented user functions (montecarlo, play and exploration\_handler on this program). To do so I configured the event filter to show only events of the type 60000100 (which is the type assigned to instrumentated user functions), and the semantic options to show the last event value (that is the values identifying the functions) in a stacked composition. Thanks to the ability of copy/paste time info from a graphic we can quickly compare different traces.

 \hyperref[fig:fig:ttt-python-funcs-seq]{Figure \ref{fig:ttt-python-funcs-seq}} shows the visualization of three executions of the tic-tac-toe, all of them with 500 iterations. The first is an schedulerless version, the second with the serial scheduler and the last one with an MPI scheduler. At the time of writing the parallel/multithreading scheduler can't be instrumented, as this is a demo section of the Paraver capabilities we have considered that leaving the parallel version out will not harm the purpose of this section. Dark blue corresponds with exploration\_handler function, white with montecarlo, red with play and light blue the time outside these three functions. The MPI version has more labels but for the current section the details aren't important. We can see on the figure that the serial scheduler has an important overhead, making it slower than the schedulerless version, but the MPI scheduler is quite faster.
 
 
\begin{figure}[h]
\includegraphics[width=\textwidth]{img/ttt_500_python_funcsG11.png}
\includegraphics[width=\textwidth]{img/ttt_500_python_funcsG12.png}
\includegraphics[width=\textwidth]{img/ttt_500_python_funcsG13.png}
\caption{Tic-tac-toe 500 iterations executions}
\label{fig:ttt-python-funcs-seq}
\end{figure}


Paraver has a wide range of configurations to visualize MPI information (see \hyperref[fig:fig:ttt-mpi-IPC]{Figure \ref{fig:ttt-mpi}} ), useful execution time or IPC. We can inspect data in timeline or tabular form but also with the aid of other tools such as the Clustering. With this tool we can cluster the results to obtain, for example, a graphic relating the executed instructions and the IPC. Thanks to this we can bypass analysis problems related with the time where, sometimes, an increase or unbalance in the workload depends on the IPC rather than the number of instructions. On \hyperref[fig:fig:ttt-mpi-IPC]{Figure \ref{fig:ttt-mpi-IPC}} we can see that the most computation-intensive areas associated with the exploration\_handler function (in blue on  \hyperref[fig:ttt-python-funcs-seq]{Figure \ref{fig:ttt-python-funcs-seq}}) have a good efficiency. 
 
 
\begin{figure}[h]
\includegraphics[width=\textwidth]{img/New_window_1_2DZoom_range_[1,6]@test.png}
\caption{Tic-tac-toe MPI information for a 500 iteration execution}
\label{fig:ttt-mpi}
\end{figure}


 
\begin{figure}[h]
\includegraphics[width=\textwidth]{img/cluster_graph_mpi.png}
\includegraphics[width=\textwidth]{img/cluster_mpi_IPC.png}
\caption{Tic-tac-toe timeline and clustering for 500 iteration MPI execution}
\label{fig:ttt-mpi-IPC}
\end{figure}


\section{Work Methodology}
\label{sec:methodology}
There are many ways to try to optimize pyProCT, some of them complex enough to be a full project. Due to the limited time and the loop structure of the development, we decided to use an Scrum based methodology. We set time-variable cycles at the end of which the work was evaluated. 

I have used Paraver and Extrae, for traces' analysis; pyProCT-regression, to validate the new implementation. These tools, as well as pyCOMPSs and pyProCT, are still on development and not fully tested. This scenario suited best an Scrum methodology. Having cycles meant that it was easier to evaluate if some trials led to a dead-end, were best implemented on another way or, simply, were not feasible because they were unsupported.

Each cycle was bound to an specific modification. To begin each cycle we analysed first the state of the project and how previous work affected the code to define the next goal and the expected results. Once decided the work plan, we proceeded to it's implementation. 

Once finished we checked if the goals were achieved. The duration of each cycle was variable because the complexity of each optimization can vary a lot. This helped to keep track of the work and decide if a particular modification was taking too long or could not be implemented. It also reflected the possibility that an optimization did not improve the overall performance, case in which the results were analysed and reported nonetheless prior to planning the next work to be done.

This methodology takes into account how each modification affects the next one allowing a better planning. We deemed this approach better than  performing a full initial analysis and deciding at once all the optimizations to be implemented. 

To work on this project I used a laptop with the text editor Sublime Text 3. The computer had installed all the required software to run the code on the Mare Nostrum machine (through ssh), fork and manage the code versions with git, run the tests and instrument the code (for further details see both \ref{sec:hardware_resources} Hardware Resources and \ref{sec:software_resources} Software Resources sections).

\subsection{Limitations and Risks}


The reproducibility problem, defined as the impossibility to repeat an exact execution of the algorithm because of some stochastic parts, such as random initial parameters estimation for example, could difficult the validation and testing part. This could lead to a number of problems. First, the inability to use the black-box validation if two executions with the same data set lead to different results. This clearly affects all the parts of the process involving some kind of randomness. To control this, in case it  affects the testing process, I will try to eliminate the stochastic issues with random seeds and manual and fixed parameter estimation.

Another issue could be the time. To mitigate this problem the initial set-up phase before the SCRUM iterations has been added (see subsection \ref{subsec:setup} on Tasks Description). The goal of this is to automate the analysis, execution and all the other time-consuming tasks not related to the actual development of the optimizations. 

More problems such as the inability to correctly enqueue jobs to Mare Nostrum III will be addressed by counting on the BSC team and the project director. The usage of extrae and paraver tools could also be difficult. To overcome it, on the one hand, I went to a seminar about that tools. On the other hand I contacted the tools team to get their help when needed. This support is taken into account as an exterior consultant. 

At the present time COMPSs team is working on a brand new release (1.3). It has important changes with respect to the last one (1.2). The goal of this project to use, test and evaluate COMPSs so 

I decided that using the development version (1.3) would be more useful for the team and its performance is better than the old one. However, this new one is neither finished nor fully documented and tested. This is probably the major source of problems for this project.

Finally, the decision to work with the development version is the biggest risk of all because I can not finish my work if the release is not stable enough to run pyProCT. 


\section{Budget}

This section describes the required budget for the pyProCT optimization project. It contains a detailed description of the material and human costs. It is divided into: human, hardware and software resources; instead of specifying the costs per tasks, we have decided to use this structure because there are not remarkable differences between the resources used for each task so, grouping them this way, the document will be clearer, will avoid too many subsections and, on the Temporal Planning, the resources needed for each task have already been specified.

\subsection{Human Resources}


The project was completed by developer and a supervisor. The first worked with an eight-long workday from Monday to Friday. The second's task was to supervise and assess the working process, give advice and help to solve issues. The estimation of the human cost is tied to the work time represented, in this case, by the Gantt Chart and the task description provided on the \hyperref[sec:gantt_pert]{\ref*{sec:gantt_pert} Gantt and PERT charts} section of the temporal planning. 


First is important to note that the Project Management task overlaps with other tasks. However the duration of this section is tied to the programmed schedule of the GEP course, not to the amount of work required to finish it. Thanks to that, we will consider that from 12th February to 12th March the workday is going to be equally distributed amongst the overlapping tasks, which are less work-intensive because they are merely familiarization and research tasks. Similarly, because the results are tied to the development of COMPSs new release, I will start writing the report before having all the results. This way I will have almost everything ready before having the results. 

The defined work period has 176 workdays from 12th February to 13th October (12d + 22d + 22d + 22d + 5d monthly breakdown) amounting to approximately a total of 1400 hours. For the supervisor we estimate 200 hours distributed between meetings, project setup, problem's resolution and correction of this document. 
\\

\begin{center}
	\begin{tabular}{| R{3cm} | L{3cm} | L{3cm} | L{3cm} |}
	\hline
	Role & Price per hour & Time & Cost \\ 
	\hline
	\hline
	Project Developer & 10,00 \euro & 1400h & 14.000,00 \euro \\
	\hline
	Project Supervisor & 30,00 \euro & 100h & 6.000,00 \euro \\
	\hline
	\hline
	Total & - & - & 20.000,00 \euro \\
	\hline
	\end{tabular}
	\captionof{table}{Human Resources Budget} 
\end{center}


\subsection{Hardware Resources}
\label{sec:hardware_resources}
The hardware resources for this software project are going to be the development device, a laptop, and the testing one, the Mare Nostrum III. It's assumed that the computer used for development has an internet connection and electrical connection. These costs are covered on the total budget together with unexpected costs. However to reduce the budget one possibility would be to consider using the university facilities. The university provides to it's students and developers a free network and plugs which is more than enough in this case.


\makesavenoteenv{tabular}
\begin{center}
	\begin{tabular}{| R{4cm} | L{3cm} | L{2cm} | L{3cm} |}
	\hline
	Product & Price & Useful life & Amortisation \\ 
	\hline \hline
	Mare Nostrum III & 22.700.000,00 \euro & 3 years & 0\protect\footnote{ MareNostrum III is a public infrastructure so users need not to pay to use it} \euro \\
	\hline
	Laptop & 1.200,00 \euro & 3 years & 150,09\footnote{ Given by: Cost / Useful life * Time used on project (664h)} \euro \\
	\hline
	\hline
	Total & 22.701.200,00 \euro & & 150,09 \euro \\
	\hline
	\end{tabular}
	\captionof{table}{Hardware Resources Budget}
\end{center}


\subsection{Software Resources}
\label{sec:software_resources}

pyProCT is an open source software hosted on a public github repository which can be used without restriction subject to the condition of citing the following article:

%TODO problem with accents in victor name
%Copyright (C) 2012 Víctor Alejandro Gil Sepúlveda
pyProCT: Automated Cluster Analysis for Structural Bioinformatics
J. Chem. Theory Comput., 2014, 10 (8), pp 3236?3243
DOI: 10.1021/ct500306s \\
As our aim is to improve this software we want to keep it as it is. This means, on one hand, that all the features and optimizations added to it will also be free and public, using no third-party paying software. On the other hand, being it a public software we have decided that the development will allow reproducible research, meaning that all the tools used for analysis are also going to be free and available to anyone trying to reproduce the analysis and optimizations of this project.



\begin{center}
	\begin{tabular}{| R{4cm} | L{3cm} | L{2cm} | L{3cm} |}
	\hline
	Product & Price & Useful life & Amortisation \\ 
	\hline \hline
	Linux Mint 17.1 & 0,00 \euro & - & 0,00 \euro \\
	\hline
	Extrae & 0,00 \euro & - & 0,00 \euro \\
	\hline
	Paraver & 0,00 \euro & - & 0,00 \euro \\
	\hline
	Git & 0,00 \euro& - & 0,00 \euro \\
	\hline
	Github account\footnote{ The repository is public so no premium account is required}& 0,00 \euro & - & 0,00 \euro \\
	\hline
	Texstudio & 0,00 \euro & - & 0,00 \euro \\
	\hline
	GanttProject & 0,00 \euro & - & 0,00 \euro \\
	\hline
	Dia2code (UML drawing)& 0,00 \euro & - & 0,00 \euro \\
	\hline
	Atenea UPC& 0,00 \euro & - & 0,00 \euro \\
	\hline
	Other tools & 0,00 \euro & - & 0,00 \euro \\
	\hline
	\hline
	Total & 0,00 \euro \euro & & 0,00 \euro \euro \\
	\hline
	\end{tabular}
	\captionof{table}{Software Resources Budget}
\end{center}



\subsection{Total Budget}

Adding up all the cost described on the previous section we get total cost of the project, to which we need to add the VAT, which is 21 \% in Spain. We do not expect big problems or incidents because, as we stated, we aim to use only free software so any modification or change on the task's planning will mainly just add office rental costs (taking into account that the office rental also includes the electricity and internet costs).

To control unexpected events we will add to the Total Cost an amount of money to confront them. These would cover various problems such as: an electricity or internet cost rise, more required office time (rising the rental costs and network/electricity) or, in case of not having enough time, the hiring of supporting help (other developers).


\begin{center}
	\begin{tabular}{| R{7cm} | L{3cm} | L{2cm} | L{3cm} |}
	\hline
	Resource & Price & Useful life & Amortisation \\ 
	\hline \hline
	Hardware & 22.701.200,00 \euro & & 150,09 \euro \\
	\hline
	Software & 0,00 \euro & & 0,00 \euro \\
	\hline
	Developers & 20.000,00 \euro & - & 20.000,00 \euro \\
	\hline
	Office rental & 5.000,00 \euro & - & 5.000,00 \euro \\
	\hline
	Unexpected costs & 3.000,00 \euro & - & 3.000,00\footnote{ Given by: Cost / Useful life * Time used on project (664h)} \euro \\
	\hline
	\hline
	Subtotal & 22.729.200,00 \euro & - & 38.011,70 \euro \\
	\hline
	VAT (21 \%) & 4.773.132,00 \euro & - & 7.982.46 \euro \\
	\hline
	\hline
	Total & 27.502.332,00 \euro & - & 45.994.16 \euro \\
	\hline
	\end{tabular}
	\captionof{table}{Total Budget}
\end{center}


\section{Problems}

\subsection{Python versions}

This first issue arose when trying to compile and link the development version of pyProCT. It is related with MN3's modules environment. By default, on login, MN3 has 2.6.9 python, however this version is not available to be loaded through the modules; it's only available when no other python has been loaded by the .bashrc file nor manually with \textit{module load PYTHON}. 

pyProCT depends on python 2.7.3 which can be loaded with the modules. At first I compiled and installed it with the default release (2.6.9) with setup.py. On MN3 I had to add a custom installation path (with \textit{--prefix=PATH} option) to setup.py because I have no permissions to write into the default installation directory. After installing it I found out that pyProCT can not be run under python 2.6.9 so I started again all the installation process with 2.7.3 once I figured what was causing the error. 

The new installation arose the next issue

\subsection{Unicode encoding}

The message error of this bug was:

\textit{undefined symbol: PyUnicodeUCS4\_DecodeUTF8}

This is caused when trying to use software build with UCS4 on a UCS2 python version. On MN3 each installation uses a different one.

\begin{itemize}
\itemsep0em 
\item Python 2.7.3 $\rightarrow$ UCS2
\item Python 2.6.9 $\rightarrow$ UCS4
\end{itemize}

Python is not a compiled language, so this compilation problem actually comes from the Cython modules integrated into pyProCT. This meant that the new installation (which used the same folder as source) was not recompiling the Cython modules even after issuing a clean command so I cloned the repo again and started from scratch. This time everything ran smoothly. As a curiosity if pyProCT is build with python2.6.9 it can be used with python2.7.3 (although rising some compatibility warnings).

\subsection{Prody and extrae}

When trying to generate traces with extrae (for MPI and sequential version) I got a Prody error. When trying to resize any kind of structure Prody detects that there is more than one reference to that structure (introduced by the instrumentation) and fails to do the resize. To avoid this I had to manually modify the Prody package and, for each resize, add the parameter \textit{refcheck=False}. This error is raised in order to avoid integrity problems when an object has more than one reference; however we know that the instrumentation will not modify nor actively use those structures so we can safely disable the reference's check. 

\subsection{Size of floats}

This issue was raised by some datasets. Depending on the computer and data when I try to recreate the condensed matrix on the pyCOMPSs task I get an incompatible format error. This happens when numpy stores the matrix data (in list format) as floats with 32 bits. The matrix constructor however requires that data to be in floats with 64 bits. To overcome this I found that numpy arrays have a method, \textit{data\_view('float64')}, to select which type of elements should be returned and thus allowing me to always format them as 64-bit floats and solve the issue.
